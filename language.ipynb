{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    Load the dataset\n",
    "    \"\"\"\n",
    "    input_file=os.path.join(path)\n",
    "    with open(input_file,\"r\") as f:\n",
    "        data=f.read()\n",
    "    return data.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_sentences=load_data('europarl-v7.fr-en.en')\n",
    "french_sentences=load_data('europarl-v7.fr-en.fr')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small_vocab_en line 1: new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "samll vocab_fr line 1: new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "small_vocab_en line 2: the united states is usually chilly during july , and it is usually freezing in november .\n",
      "samll vocab_fr line 2: les Ã©tats-unis est gÃ©nÃ©ralement froid en juillet , et il gÃ¨le habituellement en novembre .\n"
     ]
    }
   ],
   "source": [
    "for sample_i in range(2):\n",
    "    print('small_vocab_en line {}: {}'.format(sample_i + 1,english_sentences[sample_i]))\n",
    "    print('samll vocab_fr line {}: {}'.format(sample_i + 1,french_sentences[sample_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'is': 205858, ',': 140897, '.': 129039, 'in': 75525, 'it': 75137, 'during': 74933, 'the': 67628, 'but': 63987, 'and': 59850, 'sometimes': 37746, 'usually': 37507, 'never': 37500, 'least': 27564, 'favorite': 27371, 'fruit': 27105, 'most': 14934, 'loved': 13666, 'liked': 13546, 'new': 12197, 'paris': 11334, 'india': 11277, 'united': 11270, 'states': 11270, 'california': 11250, 'jersey': 11225, 'france': 11170, 'china': 10953, 'he': 10786, 'she': 10786, 'grapefruit': 10118, 'your': 9734, 'my': 9700, 'his': 9700, 'her': 9700, 'fall': 9134, 'june': 9133, 'spring': 9102, 'january': 9090, 'winter': 9038, 'march': 9023, 'autumn': 9004, 'may': 8995, 'nice': 8984, 'september': 8958, 'july': 8956, 'april': 8954, 'november': 8951, 'summer': 8948, 'december': 8945, 'february': 8942, 'our': 8932, 'their': 8932, 'freezing': 8928, 'pleasant': 8916, 'beautiful': 8915, 'october': 8910, 'snowy': 8898, 'warm': 8890, 'cold': 8878, 'wonderful': 8808, 'dry': 8794, 'busy': 8791, 'august': 8789, 'chilly': 8770, 'rainy': 8761, 'mild': 8743, 'wet': 8726, 'relaxing': 8696, 'quiet': 8693, 'hot': 8639, 'dislikes': 7314, 'likes': 7314, 'limes': 5554, 'mangoes': 5549, 'lemons': 5533, 'grapes': 5525, 'apples': 5452, 'oranges': 5452, 'strawberries': 5452, 'bananas': 5452, 'peaches': 5451, 'pears': 5451, 'to': 5166, 'strawberry': 4715, 'grape': 4703, 'lime': 4680, 'apple': 4652, 'lemon': 4652, 'banana': 4652, 'mango': 4652, 'pear': 4652, 'peach': 4652, 'orange': 4651, 'like': 4588, 'dislike': 4444, 'they': 3222, 'that': 2712, 'i': 2664, 'we': 2532, 'you': 2414, 'animal': 2304, 'a': 1944, 'truck': 1944, 'car': 1944, 'automobile': 1944, 'was': 1867, 'next': 1666, 'go': 1386, 'driving': 1296, 'visit': 1224, 'little': 1016, 'big': 1016, 'old': 972, 'yellow': 972, 'red': 972, 'rusty': 972, 'blue': 972, 'white': 972, 'black': 972, 'green': 972, 'shiny': 972, 'favorite.': 961, 'are': 870, '?': 811, 'last': 781, 'feared': 768, 'animals': 768, 'this': 768, 'plan': 714, 'going': 666, 'saw': 648, 'disliked': 648, 'drives': 648, 'drove': 648, 'grapefruit.': 574, 'between': 540, 'liked.': 500, 'loved.': 500, 'translate': 480, 'plans': 476, 'peaches.': 393, 'pears.': 393, 'bananas.': 392, 'oranges.': 392, 'apples.': 392, 'strawberries.': 392, 'were': 384, 'went': 378, 'might': 378, 'wanted': 378, 'thinks': 360, 'grapes.': 319, 'spanish': 312, 'portuguese': 312, 'chinese': 312, 'english': 312, 'french': 312, 'lemons.': 311, 'translating': 300, 'mangoes.': 295, 'limes.': 290, 'difficult': 260, 'fun': 260, 'easy': 260, 'wants': 252, 'think': 240, 'why': 240, \"it's\": 240, 'did': 204, 'orange.': 197, 'mango.': 196, 'banana.': 196, 'peach.': 196, 'lemon.': 196, 'pear.': 196, 'apple.': 196, 'cat': 192, 'shark': 192, 'bird': 192, 'mouse': 192, 'horse': 192, 'elephant': 192, 'dog': 192, 'monkey': 192, 'lion': 192, 'bear': 192, 'rabbit': 192, 'snake': 192, 'lime.': 168, 'grape.': 145, 'when': 144, 'strawberry.': 133, 'want': 126, 'fruit.': 87, 'do': 84, 'how': 67, 'elephants': 64, 'horses': 64, 'dogs': 64, 'sharks': 64, 'snakes': 64, 'cats': 64, 'rabbits': 64, 'monkeys': 64, 'bears': 64, 'birds': 64, 'lions': 64, 'mice': 64, \"didn't\": 60, 'eiffel': 57, 'tower': 57, 'grocery': 57, 'store': 57, 'football': 57, 'field': 57, 'lake': 57, 'school': 57, 'would': 48, \"aren't\": 36, 'been': 36, 'weather': 33, 'does': 24, 'has': 24, \"isn't\": 24, 'am': 24, 'where': 12, 'have': 12}) total english words\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "english_words_counter=collections.Counter([word for sentence in english_sentences for word in sentence.split() ])\n",
    "#print the count for each corresponding word\n",
    "french_words_counter=collections.Counter([word for sentence in french_sentences for word in sentence.split() ])\n",
    "\n",
    "print('{} total english words'.format(english_words_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1823250 english words\n",
      "227 unique english words\n",
      "1961295 french words\n",
      "355 unique french words\n",
      "total sentences in french {}: 137861\n",
      "total sentences in english {}:  137861\n"
     ]
    }
   ],
   "source": [
    "ctr=0\n",
    "print(\"{} english words\".format(len([word for sentence in english_sentences for word in sentence.split()])))\n",
    "print(\"{} unique english words\".format(len(english_words_counter)))\n",
    "print(\"{} french words\".format(len([word for sentence in french_sentences for word in sentence.split()])))\n",
    "print(\"{} unique french words\".format(len(french_words_counter)))\n",
    "print(\"total sentences in french {}:\",format(len([sentence for sentence in french_sentences])))\n",
    "print(\"total sentences in english {}: \",format(len([sentence for sentence in french_sentences])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np###########################3\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "def _test_model(model, input_shape, output_sequence_length, french_vocab_size):\n",
    "    if isinstance(model, Sequential):\n",
    "        model = model.model\n",
    "\n",
    "    assert model.input_shape == (None, *input_shape[1:]),\\\n",
    "        'Wrong input shape. Found input shape {} using parameter input_shape={}'.format(model.input_shape, input_shape)\n",
    "\n",
    "    assert model.output_shape == (None, output_sequence_length, french_vocab_size),\\\n",
    "        'Wrong output shape. Found output shape {} using parameters output_sequence_length={} and french_vocab_size={}'\\\n",
    "            .format(model.output_shape, output_sequence_length, french_vocab_size)\n",
    "\n",
    "    assert len(model.loss_functions) > 0,\\\n",
    "        'No loss function set.  Apply the `compile` function to the model.'\n",
    "\n",
    "    assert sparse_categorical_crossentropy in model.loss_functions,\\\n",
    "        'Not using `sparse_categorical_crossentropy` function for loss.'\n",
    "\n",
    "\n",
    "def test_tokenize(tokenize):\n",
    "    sentences = [\n",
    "        'The quick brown fox jumps over the lazy dog .',\n",
    "        'By Jove , my quick study of lexicography won a prize .',\n",
    "        'This is a short sentence .']\n",
    "    tokenized_sentences, tokenizer = tokenize(sentences)\n",
    "    assert tokenized_sentences == tokenizer.texts_to_sequences(sentences),\\\n",
    "        'Tokenizer returned and doesn\\'t generate the same sentences as the tokenized sentences returned. '\n",
    "\n",
    "\n",
    "def test_pad(pad):\n",
    "    tokens = [\n",
    "        [i for i in range(4)],\n",
    "        [i for i in range(6)],\n",
    "        [i for i in range(3)]]\n",
    "    padded_tokens = pad(tokens)\n",
    "    padding_id = padded_tokens[0][-1]\n",
    "    true_padded_tokens = np.array([\n",
    "        [i for i in range(4)] + [padding_id]*2,\n",
    "        [i for i in range(6)],\n",
    "        [i for i in range(3)] + [padding_id]*3])\n",
    "    assert isinstance(padded_tokens, np.ndarray),\\\n",
    "        'Pad returned the wrong type.  Found {} type, expected numpy array type.'\n",
    "    assert np.all(padded_tokens == true_padded_tokens), 'Pad returned the wrong results.'\n",
    "\n",
    "    padded_tokens_using_length = pad(tokens, 9)\n",
    "    assert np.all(padded_tokens_using_length == np.concatenate((true_padded_tokens, np.full((3, 3), padding_id)), axis=1)),\\\n",
    "        'Using length argument return incorrect results'\n",
    "\n",
    "\n",
    "def test_simple_model(simple_model):\n",
    "    input_shape = (137861, 21, 1)\n",
    "    output_sequence_length = 21\n",
    "    english_vocab_size = 199\n",
    "    french_vocab_size = 344\n",
    "\n",
    "    model = simple_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size)\n",
    "    _test_model(model, input_shape, output_sequence_length, french_vocab_size)\n",
    "\n",
    "\n",
    "def test_embed_model(embed_model):\n",
    "    input_shape = (137861, 21)\n",
    "    output_sequence_length = 21\n",
    "    english_vocab_size = 199\n",
    "    french_vocab_size = 344\n",
    "\n",
    "    model = embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size)\n",
    "    _test_model(model, input_shape, output_sequence_length, french_vocab_size)\n",
    "\n",
    "\n",
    "def test_encdec_model(encdec_model):\n",
    "    input_shape = (137861, 15, 1)\n",
    "    output_sequence_length = 21\n",
    "    english_vocab_size = 199\n",
    "    french_vocab_size = 344\n",
    "\n",
    "    model = encdec_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size)\n",
    "    _test_model(model, input_shape, output_sequence_length, french_vocab_size)\n",
    "\n",
    "\n",
    "def test_bd_model(bd_model):\n",
    "    input_shape = (137861, 21, 1)\n",
    "    output_sequence_length = 21\n",
    "    english_vocab_size = 199\n",
    "    french_vocab_size = 344\n",
    "\n",
    "    model = bd_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size)\n",
    "    _test_model(model, input_shape, output_sequence_length, french_vocab_size)\n",
    "\n",
    "\n",
    "def test_model_final(model_final):\n",
    "    input_shape = (137861, 15)\n",
    "    output_sequence_length = 21\n",
    "    english_vocab_size = 199\n",
    "    french_vocab_size = 344\n",
    "\n",
    "    model = model_final(input_shape, output_sequence_length, english_vocab_size, french_vocab_size)\n",
    "    _test_model(model, input_shape, output_sequence_length, french_vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 1, 'quick': 2, 'a': 3, 'brown': 4, 'fox': 5, 'jumps': 6, 'over': 7, 'lazy': 8, 'dog': 9, 'by': 10, 'jove': 11, 'my': 12, 'study': 13, 'of': 14, 'lexicography': 15, 'won': 16, 'prize': 17, 'this': 18, 'is': 19, 'short': 20, 'sentence': 21}\n",
      "\n",
      "Sequence 1 in x\n",
      "Input: The quick brown fox jumps over the lazy dog .\n",
      "Output [1, 2, 4, 5, 6, 7, 1, 8, 9]\n",
      "Sequence 2 in x\n",
      "Input: By Jove , my quick study of lexicography won a prize .\n",
      "Output [10, 11, 12, 2, 13, 14, 15, 16, 3, 17]\n",
      "Sequence 3 in x\n",
      "Input: This is a short sentence .\n",
      "Output [18, 19, 3, 20, 21]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "def tokenize(x):\n",
    "    tokenizer=Tokenizer()\n",
    "    tokenizer.fit_on_texts(x)\n",
    "    return tokenizer.texts_to_sequences(x),tokenizer\n",
    "test_tokenize(tokenize)\n",
    "# Tokenize Example output\n",
    "text_sentences = [\n",
    "    'The quick brown fox jumps over the lazy dog .',\n",
    "    'By Jove , my quick study of lexicography won a prize .',\n",
    "    'This is a short sentence .']\n",
    "text_tokenized,text_tokenizer=tokenize(text_sentences)\n",
    "print(text_tokenizer.word_index)\n",
    "print()\n",
    "for sample_i,(sent,token_sent) in enumerate(zip(text_sentences,text_tokenized)):\n",
    "    print('Sequence {} in x'.format(sample_i+1))\n",
    "    print('Input: {}'.format(sent))\n",
    "    print('Output {}'.format(token_sent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1 in x\n",
      "Input [1 2 4 5 6 7 1 8 9]\n",
      "Output: [1 2 4 5 6 7 1 8 9 0]\n",
      "Sequence 2 in x\n",
      "Input [10 11 12  2 13 14 15 16  3 17]\n",
      "Output: [10 11 12  2 13 14 15 16  3 17]\n",
      "Sequence 3 in x\n",
      "Input [18 19  3 20 21]\n",
      "Output: [18 19  3 20 21  0  0  0  0  0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nalist = ['a1', 'a2', 'a3']\\nblist = ['b1', 'b2', 'b3']\\n\\nfor a, b in zip(alist, blist):\\n    print a, b\\n    results:\\na1 b1\\na2 b2\\na3 b3\\nalist = ['a1', 'a2', 'a3']\\n\\nfor i, a in enumerate(alist):\\n    print i, a\\nResults:\\n\\n0 a1\\n1 a2\\n2 a3\\n    \\n    \""
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "def pad(x,length=None):\n",
    "    return pad_sequences(x,maxlen=length,padding='post')\n",
    "test_pad(pad)\n",
    "test_pad=pad(text_tokenized)\n",
    "for sample_i,(token_sent,pad_sent) in enumerate(zip(text_tokenized,test_pad)):\n",
    "    print('Sequence {} in x'.format(sample_i+1))\n",
    "    print('Input {}'.format(np.array(token_sent)))\n",
    "    print('Output: {}'.format(pad_sent))\n",
    "\"\"\"\n",
    "alist = ['a1', 'a2', 'a3']\n",
    "blist = ['b1', 'b2', 'b3']\n",
    "\n",
    "for a, b in zip(alist, blist):\n",
    "    print a, b\n",
    "    results:\n",
    "a1 b1\n",
    "a2 b2\n",
    "a3 b3\n",
    "alist = ['a1', 'a2', 'a3']\n",
    "\n",
    "for i, a in enumerate(alist):\n",
    "    print i, a\n",
    "Results:\n",
    "\n",
    "0 a1\n",
    "1 a2\n",
    "2 a3\n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape before: (137861, 21)\n",
      "shpe after: 1\n",
      "DAta preprocessed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nshape before:1378621 is the total no of sentences,\\n21 is the total words in 1 sentences ---we are doing it only for french'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(x,y):#combination of the above two functions\n",
    "    preprocess_x,x_tk=tokenize(x)\n",
    "    preprocess_y,y_tk=tokenize(y)\n",
    "    preprocess_x=pad(preprocess_x)\n",
    "    preprocess_y=pad(preprocess_y)##preprocess_x after padding,x_tk=words in all the sentences\n",
    "    print('shape before:',preprocess_y.shape)#preprocess_y after reshaping\n",
    "    preprocess_y=preprocess_y.reshape(*preprocess_y.shape,1)\n",
    "    print('shpe after:',preprocess_y.shape[-1])#shape 0:13861,shape 1:21....&&& shape -1:last,shape-2:second last........\n",
    "    return(preprocess_x,preprocess_y,x_tk,y_tk)\n",
    "preproc_english_sentences,preproc_french_sentences,english_tokenizer,french_tokenizer=preprocess(english_sentences,french_sentences)\n",
    "print('DAta preprocessed')\n",
    "\"\"\"\n",
    "shape before:1378621 is the total no of sentences,\n",
    "21 is the total words in 1 sentences ---we are doing it only for french\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits_to_ text loaded.................\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\npred = np.array([[31, 23,  4, 24, 27, 34],\\n                [18,  3, 25,  0,  6, 35],\\n                [28, 14, 33, 22, 20,  8],\\n                [13, 30, 21, 19,  7,  9],\\n                [16,  1, 26, 32,  2, 29],\\n                [17, 12,  5, 11, 10, 15]])\\n\\ny = np.array([[31, 23,  4, 24, 27, 34],\\n                [18,  3, 25,  0,  6, 35],\\n                [28, 14, 33, 22, 20,  8],\\n                [13, 30, 21, 19,  7,  9],\\n                [16,  1, 26, 32,  2, 29],\\n                [17, 12,  5, 11, 10, 15]])\\nEvaluating tf.argmax(pred, 1) gives a tensor whose evaluation will give array([5, 5, 2, 1, 3, 0])\\n'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#######################to see the output properly we r converting indices into words\n",
    "def logits_to_text(logits,tokenizer):\n",
    "    index_to_words={id:word for word,id in tokenizer.word_index.items()}\n",
    "    index_to_words[0]='<PAD>'\n",
    "    return' '.join([index_to_words[prediction] for prediction in np.argmax(logits,1)])\n",
    "##argmax at wich th efunction output are as large as possible\n",
    "print('logits_to_ text loaded.................')\n",
    "\"\"\"\n",
    "pred = np.array([[31, 23,  4, 24, 27, 34],\n",
    "                [18,  3, 25,  0,  6, 35],\n",
    "                [28, 14, 33, 22, 20,  8],\n",
    "                [13, 30, 21, 19,  7,  9],\n",
    "                [16,  1, 26, 32,  2, 29],\n",
    "                [17, 12,  5, 11, 10, 15]])\n",
    "\n",
    "y = np.array([[31, 23,  4, 24, 27, 34],\n",
    "                [18,  3, 25,  0,  6, 35],\n",
    "                [28, 14, 33, 22, 20,  8],\n",
    "                [13, 30, 21, 19,  7,  9],\n",
    "                [16,  1, 26, 32,  2, 29],\n",
    "                [17, 12,  5, 11, 10, 15]])\n",
    "Evaluating tf.argmax(pred, 1) gives a tensor whose evaluation will give array([5, 5, 2, 1, 3, 0])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(137861, 21, 1)\n",
      "(137861, 15)\n",
      "french 1\n",
      "[[ 35]\n",
      " [ 34]\n",
      " [  1]\n",
      " [  8]\n",
      " [ 67]\n",
      " [ 37]\n",
      " [ 11]\n",
      " [ 24]\n",
      " [  6]\n",
      " [  3]\n",
      " [  1]\n",
      " [112]\n",
      " [  2]\n",
      " [ 50]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]]\n",
      "english vocab size 199\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "#basic RNN\n",
    "import os\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "print(preproc_french_sentences.shape)\n",
    "print(preproc_english_sentences.shape)\n",
    "print(\"french\",preproc_french_sentences.shape[2])\n",
    "print(preproc_french_sentences[0])\n",
    "print(\"english vocab size\",len(english_tokenizer.word_index) )\n",
    "print(preproc_french_sentences.shape[-2])#french we have reshaped before check the outputs for -3,-2,-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(137861, 15)\n",
      "[[17 23  1 ... 44  0  0]\n",
      " [ 5 20 21 ... 51  2 45]\n",
      " [22  1  9 ... 34  0  0]\n",
      " ...\n",
      " [24  1 10 ... 54  0  0]\n",
      " [ 5 84  1 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]]\n",
      "[[17 23  1 ...  0  0  0]\n",
      " [ 5 20 21 ...  0  0  0]\n",
      " [22  1  9 ...  0  0  0]\n",
      " ...\n",
      " [24  1 10 ...  0  0  0]\n",
      " [ 5 84  1 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]]\n",
      "input shape:  (137861, 21, 1)\n"
     ]
    }
   ],
   "source": [
    "#reshaping the english sentence \n",
    "print(preproc_english_sentences.shape)\n",
    "print(preproc_english_sentences)\n",
    "# earlier  english sentences padding was 15,now we increased the padding to 21 equal to that of french,,done below \n",
    "tmp_x=pad(preproc_english_sentences,preproc_french_sentences.shape[1])\n",
    "print(tmp_x)\n",
    "tmp_x=tmp_x.reshape((-1,preproc_french_sentences.shape[-2],1))\n",
    "print(\"input shape: \", tmp_x.shape)#english sentence padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_5 (GRU)                  (None, 21, 128)           49920     \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 21, 344)           44376     \n",
      "=================================================================\n",
      "Total params: 94,296\n",
      "Trainable params: 94,296\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VIDUSHI\\anaconda\\lib\\site-packages\\keras\\engine\\sequential.py:109: UserWarning: `Sequential.model` is deprecated. `Sequential` is a subclass of `Model`, you can just use your `Sequential` instance directly.\n",
      "  warnings.warn('`Sequential.model` is deprecated. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_6 (GRU)                  (None, 21, 128)           49920     \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 21, 346)           44634     \n",
      "=================================================================\n",
      "Total params: 94,554\n",
      "Trainable params: 94,554\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/5\n",
      "110288/110288 [==============================] - 168s 2ms/step - loss: 2.2438 - acc: 0.5520 - val_loss: 1.6812 - val_acc: 0.5971\n",
      "Epoch 2/5\n",
      "110288/110288 [==============================] - 172s 2ms/step - loss: 1.5758 - acc: 0.6246 - val_loss: 1.6356 - val_acc: 0.5957\n",
      "Epoch 3/5\n",
      "110288/110288 [==============================] - 171s 2ms/step - loss: 1.5053 - acc: 0.6318 - val_loss: 1.7339 - val_acc: 0.5922\n",
      "Epoch 4/5\n",
      "110288/110288 [==============================] - 173s 2ms/step - loss: 1.4633 - acc: 0.6350 - val_loss: 1.8224 - val_acc: 0.5726\n",
      "Epoch 5/5\n",
      "110288/110288 [==============================] - 170s 2ms/step - loss: 1.4714 - acc: 0.6349 - val_loss: 1.7276 - val_acc: 0.5908\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-8a6771ce869e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0msimple_rnn_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cache\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"simple_model.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msimple_rnn_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msimple_rnn_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "# finally making a model--model1\n",
    "from keras.layers import GRU, Input, Dense, TimeDistributed\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "#output_seq.len-max frech words(21)\n",
    "#this we are doing for a single sentence\n",
    "def simple_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size, learning_rate=0.1):\n",
    "    #french_vocab_size---total unique frech words\n",
    "    model=Sequential()\n",
    "    model.add(GRU(128,dropout=0.1,input_shape=input_shape[1:],return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size,activation='softmax')))\n",
    "    #time based back prop \n",
    "    model.summary()\n",
    "     \n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "test_simple_model(simple_model)\n",
    "simple_rnn_model=simple_model(tmp_x.shape, preproc_french_sentences.shape[1],len(english_tokenizer.word_index)+1,\n",
    "    len(french_tokenizer.word_index)+1)#word_index: A dictionary of words and their uniquely assigned integers.\n",
    "if os.path.exists(os.path.join(\"cache\", \"simple_model.h5\"))== False:\n",
    "    simple_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=5, validation_split=0.2)\n",
    "else:\n",
    "    simple_rnn_model = load_model(os.path.join(\"cache\", \"simple_model.h5\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_rnn_model.save(\"simple_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.4927151e-06 6.3669737e-05 1.6500009e-05 ... 8.4701897e-15\n",
      "  3.9423565e-15 1.6375237e-15]\n",
      " [2.7375845e-07 6.1663563e-06 5.2897990e-06 ... 2.2512842e-17\n",
      "  3.0490738e-17 1.1022392e-17]\n",
      " [7.4483627e-03 9.6897829e-01 2.1146795e-04 ... 8.4771573e-22\n",
      "  9.8846411e-21 8.6875374e-20]\n",
      " ...\n",
      " [9.9971873e-01 1.6171141e-06 3.1570228e-06 ... 1.8362580e-19\n",
      "  1.1098667e-18 4.7598030e-19]\n",
      " [9.9992812e-01 2.6705681e-07 9.7763791e-07 ... 2.3704978e-20\n",
      "  1.4060021e-19 5.2977794e-20]\n",
      " [9.9994409e-01 1.5604728e-07 8.3260898e-07 ... 1.1432768e-20\n",
      "  6.5921170e-20 2.3400778e-20]]\n",
      "new jersey est parfois chaud en juin juin il est agrã©able en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Train accurancy:  0.5905589515563147\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#prediction\n",
    "#tmp_x[:1]-all the words'indices' in all the sentences ,see the dimension above\n",
    "print(simple_rnn_model.predict(tmp_x[:1])[0])##this will be passed as logits in logits_to_text\n",
    "print(logits_to_text(simple_rnn_model.predict(tmp_x[:1])[0],french_tokenizer))\n",
    "score = simple_rnn_model.evaluate(tmp_x, preproc_french_sentences, verbose=0)\n",
    "print(\"Train accurancy: \", score[1])\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 21, 32)            11008     \n",
      "_________________________________________________________________\n",
      "gru_7 (GRU)                  (None, 21, 128)           61824     \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 21, 344)           44376     \n",
      "=================================================================\n",
      "Total params: 117,208\n",
      "Trainable params: 117,208\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VIDUSHI\\anaconda\\lib\\site-packages\\keras\\engine\\sequential.py:109: UserWarning: `Sequential.model` is deprecated. `Sequential` is a subclass of `Model`, you can just use your `Sequential` instance directly.\n",
      "  warnings.warn('`Sequential.model` is deprecated. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 21, 32)            11072     \n",
      "_________________________________________________________________\n",
      "gru_8 (GRU)                  (None, 21, 128)           61824     \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 21, 346)           44634     \n",
      "=================================================================\n",
      "Total params: 117,530\n",
      "Trainable params: 117,530\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/5\n",
      "110288/110288 [==============================] - 186s 2ms/step - loss: 12.0975 - acc: 0.1354 - val_loss: 11.7111 - val_acc: 0.1897\n",
      "Epoch 2/5\n",
      "110288/110288 [==============================] - 188s 2ms/step - loss: 12.4055 - acc: 0.1618 - val_loss: 12.5413 - val_acc: 0.1674\n",
      "Epoch 3/5\n",
      "110288/110288 [==============================] - 184s 2ms/step - loss: 12.5699 - acc: 0.1664 - val_loss: 12.4903 - val_acc: 0.1769\n",
      "Epoch 4/5\n",
      "110288/110288 [==============================] - 193s 2ms/step - loss: 12.5119 - acc: 0.1754 - val_loss: 12.4391 - val_acc: 0.1836\n",
      "Epoch 5/5\n",
      " 45056/110288 [===========>..................] - ETA: 1:35 - loss: 12.5093 - acc: 0.1782"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-dab59d6a6b8e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m     len(french_tokenizer.word_index)+1)\n\u001b[0;32m     23\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cache\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"embed_model.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0membed_rnn_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreproc_french_sentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0membed_rnn_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cache\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"embed_model.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\anaconda\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2659\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2661\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2662\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2663\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2630\u001b[0m                                 session)\n\u001b[1;32m-> 2631\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2632\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[1;32m-> 1451\u001b[1;33m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[0;32m   1452\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#model 2 with embedding in above we haven't used embedding\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import LSTM\n",
    "def embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size, learning_rate=0.1):\n",
    "    model=Sequential()\n",
    "    #Embedding(input_dim, output_dim)\n",
    "    #32-features\n",
    "    #max(english_vocab_size,french_vocab_size)-max no of examples\n",
    "    model.add(Embedding(max(english_vocab_size,french_vocab_size),32,input_length=output_sequence_length))\n",
    "    model.add(GRU(128,dropout=0.1,return_sequences=True))#Gru takes less time we can use LSTM instead\n",
    "    #model.add(LSTM(128,dropout=0.1,return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size,activation='softmax')))\n",
    "    model.summary()\n",
    "    model.compile(loss=sparse_categorical_crossentropy,optimizer=Adam(learning_rate),metrics=['accuracy'])\n",
    "    return model\n",
    "test_embed_model(embed_model)\n",
    "tmp_x=pad(preproc_english_sentences,preproc_french_sentences.shape[1])\n",
    "embed_rnn_model = embed_model(\n",
    "    tmp_x.shape,\n",
    "    preproc_french_sentences.shape[1],\n",
    "    len(english_tokenizer.word_index)+1,\n",
    "    len(french_tokenizer.word_index)+1)\n",
    "if os.path.exists(os.path.join(\"cache\", \"embed_model.h5\"))== False:\n",
    "    embed_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=5, validation_split=0.2)\n",
    "else:\n",
    "    embed_rnn_model = load_model(os.path.join(\"cache\", \"embed_model.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_rnn_model.save('embed_model.h5')\n",
    "print(logits_to_text(embed_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))\n",
    "score=embed_rnn_model.evaluate(tmp_x,preproc_french_sentences,verbose=0)\n",
    "print(\"Train accuracy\",score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_1 (Bidirection (None, 21, 256)           99840     \n",
      "_________________________________________________________________\n",
      "time_distributed_13 (TimeDis (None, 21, 344)           88408     \n",
      "=================================================================\n",
      "Total params: 188,248\n",
      "Trainable params: 188,248\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(137861, 21, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VIDUSHI\\anaconda\\lib\\site-packages\\keras\\engine\\sequential.py:109: UserWarning: `Sequential.model` is deprecated. `Sequential` is a subclass of `Model`, you can just use your `Sequential` instance directly.\n",
      "  warnings.warn('`Sequential.model` is deprecated. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_2 (Bidirection (None, 21, 256)           99840     \n",
      "_________________________________________________________________\n",
      "time_distributed_14 (TimeDis (None, 21, 346)           88922     \n",
      "=================================================================\n",
      "Total params: 188,762\n",
      "Trainable params: 188,762\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/5\n",
      " 34816/110288 [========>.....................] - ETA: 2:58 - loss: 4.5540 - acc: 0.4644"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-afe586556086>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cache\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"bd_model.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mbd_rnn_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreproc_french_sentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mbd_rnn_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cache\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"bd_model.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\anaconda\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2659\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2661\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2662\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2663\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2630\u001b[0m                                 session)\n\u001b[1;32m-> 2631\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2632\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[1;32m-> 1451\u001b[1;33m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[0;32m   1452\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#bidirectional implementation\n",
    "learning_rate=0.1\n",
    "from keras.layers import Bidirectional\n",
    "def bd_model(input_shape,output_sequence_length,english_vocab_size,french_vocab_size):\n",
    "    model=Sequential()\n",
    "    model.add(Bidirectional(GRU(128,return_sequences=True,dropout=0.1),input_shape=input_shape[1:]))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size,activation='softmax')))\n",
    "    #If instead timedistributed we write normal dense it wont compile becoz Dense expects a 2-dimensional input (batch_size, features), whereas the output of LSTM with return_sequences is 3 dimensional (batch_size, timesteps, features).\n",
    "    model.summary()\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "test_bd_model(bd_model)\n",
    "print(tmp_x.shape)\n",
    "bd_rnn_model=bd_model(tmp_x.shape,preproc_french_sentences.shape[1],len(english_tokenizer.word_index)+1,len(french_tokenizer.word_index)+1)\n",
    "\n",
    "if os.path.exists(os.path.join(\"cache\", \"bd_model.h5\"))== False:\n",
    "    bd_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=5, validation_split=0.2)\n",
    "else:\n",
    "    bd_rnn_model = load_model(os.path.join(\"cache\", \"bd_model.h5\"))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd_rnn_model.save(bd_model.h5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score=bd_rnn_model.evaluate(tmp_x,preproc_french_sentences,verbose=0)\n",
    "print('training accuracy',score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets start with the main encoding decoding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**summary**\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_29 (GRU)                 (None, 128)               49920     \n",
      "_________________________________________________________________\n",
      "repeat_vector_10 (RepeatVect (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "gru_30 (GRU)                 (None, 21, 128)           98688     \n",
      "_________________________________________________________________\n",
      "time_distributed_18 (TimeDis (None, 21, 344)           44376     \n",
      "=================================================================\n",
      "Total params: 192,984\n",
      "Trainable params: 192,984\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(137861, 21)\n",
      "(137861, 21, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VIDUSHI\\anaconda\\lib\\site-packages\\keras\\engine\\sequential.py:109: UserWarning: `Sequential.model` is deprecated. `Sequential` is a subclass of `Model`, you can just use your `Sequential` instance directly.\n",
      "  warnings.warn('`Sequential.model` is deprecated. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**summary**\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_31 (GRU)                 (None, 128)               49920     \n",
      "_________________________________________________________________\n",
      "repeat_vector_11 (RepeatVect (None, 21, 128)           0         \n",
      "_________________________________________________________________\n",
      "gru_32 (GRU)                 (None, 21, 128)           98688     \n",
      "_________________________________________________________________\n",
      "time_distributed_19 (TimeDis (None, 21, 346)           44634     \n",
      "=================================================================\n",
      "Total params: 193,242\n",
      "Trainable params: 193,242\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import RepeatVector\n",
    "\n",
    "def encdec_model(input_shape, output_sequence_length,english_vocab_size,  french_vocab_size, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    Build and train an encoder-decoder model on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # OPTIONAL: Implementif os.path.exists(\n",
    "    model = Sequential()\n",
    "    #encoder\n",
    "    #model.add(GRU(input_shape[1], input_shape=input_shape[1:], return_sequences=False) )\n",
    "    model.add(GRU(128, input_shape=input_shape[1:], return_sequences=False ))\n",
    "    model.add(RepeatVector(output_sequence_length))\n",
    "    #decoder\n",
    "    #model.add(GRU(output_sequence_length, return_sequences=True) )\n",
    "    model.add(GRU(128, return_sequences=True) )\n",
    "    #output\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax') ))\n",
    "    \n",
    "    print('**summary**')\n",
    "    model.summary()\n",
    "    \n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "test_encdec_model(encdec_model)\n",
    "#we are doing padding of english sentences once again\n",
    "print(preproc_english_sentences.shape)\n",
    "tmp_x=preproc_english_sentences,preproc_french_sentences.shape[1]\n",
    "\n",
    "tmp_x=preproc_english_sentences.reshape(preproc_english_sentences.shape[0],preproc_french_sentences.shape[1],1)\n",
    "#remember we can just change the dimension directly using reshape not inc. it for that we have to use padding first\n",
    "print(tmp_x.shape)\n",
    "encdec_model=encdec_model(tmp_x.shape,preproc_french_sentences.shape[1],len(english_tokenizer.word_index)+1,len(french_tokenizer.word_index)+1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/5\n",
      "110288/110288 [==============================] - 279s 3ms/step - loss: 3.0220 - acc: 0.4313 - val_loss: 2.9775 - val_acc: 0.4498\n",
      "Epoch 2/5\n",
      "110288/110288 [==============================] - 269s 2ms/step - loss: 2.2663 - acc: 0.4947 - val_loss: 1.9148 - val_acc: 0.5391\n",
      "Epoch 3/5\n",
      "110288/110288 [==============================] - 250s 2ms/step - loss: 1.8040 - acc: 0.5518 - val_loss: 1.6770 - val_acc: 0.5755\n",
      "Epoch 4/5\n",
      "110288/110288 [==============================] - 268s 2ms/step - loss: 1.6061 - acc: 0.5821 - val_loss: 1.5564 - val_acc: 0.5869\n",
      "Epoch 5/5\n",
      "110288/110288 [==============================] - 272s 2ms/step - loss: 1.5081 - acc: 0.5937 - val_loss: 1.4834 - val_acc: 0.5942\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(os.path.join(\"cache\",\"encdec_model.h5\"))==False:#no such file is present we have to train from the start and then save it\n",
    "    print(\"train\")\n",
    "    encdec_model.fit(tmp_x,preproc_french_sentences,batch_size=1024,epochs=5,validation_split=0.2)\n",
    "    encdec_model.save(\"encdec_model.h5\")\n",
    "else:\n",
    "    print(\"load the pretrained model\")\n",
    "    load_model(\"encdec_model.h5\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new jersey est parfois parfois en en et il il est est en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "train accuracy 0.5941650678839426\n"
     ]
    }
   ],
   "source": [
    "#lets predict the results\n",
    "\n",
    "print(logits_to_text(encdec_model.predict(tmp_x[:5])[0],french_tokenizer))\n",
    "score=encdec_model.evaluate(tmp_x,preproc_french_sentences,verbose=0)\n",
    "print(\"train accuracy\",score[1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**summary**\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_28 (Embedding)     (None, 15, 128)           44032     \n",
      "_________________________________________________________________\n",
      "bidirectional_53 (Bidirectio (None, 256)               197376    \n",
      "_________________________________________________________________\n",
      "repeat_vector_36 (RepeatVect (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_54 (Bidirectio (None, 21, 256)           295680    \n",
      "_________________________________________________________________\n",
      "time_distributed_44 (TimeDis (None, 21, 344)           88408     \n",
      "=================================================================\n",
      "Total params: 625,496\n",
      "Trainable params: 625,496\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Final Model Loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VIDUSHI\\anaconda\\lib\\site-packages\\keras\\engine\\sequential.py:109: UserWarning: `Sequential.model` is deprecated. `Sequential` is a subclass of `Model`, you can just use your `Sequential` instance directly.\n",
      "  warnings.warn('`Sequential.model` is deprecated. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**summary**\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_29 (Embedding)     (None, 15, 128)           44288     \n",
      "_________________________________________________________________\n",
      "bidirectional_55 (Bidirectio (None, 256)               197376    \n",
      "_________________________________________________________________\n",
      "repeat_vector_37 (RepeatVect (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_56 (Bidirectio (None, 21, 256)           295680    \n",
      "_________________________________________________________________\n",
      "time_distributed_45 (TimeDis (None, 21, 346)           88922     \n",
      "=================================================================\n",
      "Total params: 626,266\n",
      "Trainable params: 626,266\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "train\n",
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/5\n",
      "110288/110288 [==============================] - 605s 5ms/step - loss: 2.0968 - acc: 0.5185 - val_loss: 1.3396 - val_acc: 0.6407\n",
      "Epoch 2/5\n",
      "110288/110288 [==============================] - 1480s 13ms/step - loss: 1.0148 - acc: 0.7201 - val_loss: 0.7689 - val_acc: 0.7802\n",
      "Epoch 3/5\n",
      "110288/110288 [==============================] - 824s 7ms/step - loss: 0.5461 - acc: 0.8409 - val_loss: 0.3800 - val_acc: 0.8915\n",
      "Epoch 4/5\n",
      "110288/110288 [==============================] - 1230s 11ms/step - loss: 0.2869 - acc: 0.9191 - val_loss: 0.2433 - val_acc: 0.9318\n",
      "Epoch 5/5\n",
      "110288/110288 [==============================] - 1512s 14ms/step - loss: 0.1964 - acc: 0.9451 - val_loss: 0.1772 - val_acc: 0.9511\n"
     ]
    }
   ],
   "source": [
    "# as we can see there was ample growth by adding the embedding layer  and encoder ,decoder\n",
    "#lets see if activation functions have some effect, then adding layers\n",
    "def model_final(input_shape, output_sequence_length, english_vocab_size, french_vocab_size, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    Build and train a model that incorporates embedding, encoder-decoder, and bidirectional RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    vocab_size = max(english_vocab_size, french_vocab_size)\n",
    "    # OPTIONAL: Implement\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size ,128 , input_length=input_shape[1]))\n",
    "    #encoder\n",
    "    model.add(Bidirectional(GRU(128, return_sequences=False)) )\n",
    "    model.add(RepeatVector(output_sequence_length))\n",
    "    #decoder\n",
    "    model.add(Bidirectional(GRU(128, return_sequences=True)) )\n",
    "    #output\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax') ))\n",
    "    \n",
    "    print('**summary**')\n",
    "    model.summary()\n",
    "    \n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "test_model_final(model_final)\n",
    "\n",
    "\n",
    "print('Final Model Loaded')\n",
    "X_input = pad(preproc_english_sentences)\n",
    "model_final=model_final(\n",
    "    X_input.shape,\n",
    "    preproc_french_sentences.shape[1],\n",
    "    len(english_tokenizer.word_index)+1,\n",
    "    len(french_tokenizer.word_index)+1)\n",
    "\n",
    "if os.path.exists(os.path.join(\"cache\", \"model_final.h5\"))== False:\n",
    "    print(\"train\")\n",
    "    history=model_final.fit(X_input, preproc_french_sentences, batch_size=1024, epochs=5, validation_split=0.2)\n",
    "else:\n",
    "    print(\"load\")\n",
    "    model_final = load_model(os.path.join(\"cache\", \"model_final.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xd8VHXW+PHPSYGQBAIkoYVAIICCgJTQLWBXVqyLgKCui9jbiru6utbnt+4+Pmt3saArKlVRQEVBVFAXlQ4C0kJLiEAIpJM65/fHHUIIAQbM5E6S83698mJm7r0zJ5fMPffbRVUxxhhjAILcDsAYY0zgsKRgjDGmjCUFY4wxZSwpGGOMKWNJwRhjTBlLCsYYY8pYUjB1ioi8IyL/4+O+20XkAn/HZEwgsaRgjDGmjCUFY2ogEQlxOwZTO1lSMAHHW23zoIisEZE8EXlLRJqLyOcikiMiC0SkSbn9h4nIOhHJFJGFItK53LaeIrLCe9x0IKzCZ/1ORFZ5j10sIt19jHGoiKwUkWwRSRGRJypsP8v7fpne7Td5X28gIv8SkR0ikiUi33tfGywiqZWchwu8j58QkQ9F5H0RyQZuEpG+IvKD9zN+FZFXRKReuePPEJEvRWS/iOwRkb+KSAsRyReR6HL79RaRdBEJ9eV3N7WbJQUTqK4BLgQ6AZcDnwN/BWJw/m7vARCRTsBU4D4gFpgLfCIi9bwXyFnAe0BT4APv++I9thfwNnArEA28DswRkfo+xJcH3AA0BoYCt4vIld73beON92VvTD2AVd7j/g/oDQz0xvRnwOPjObkC+ND7mZOBUuB+7zkZAJwP3OGNoSGwAPgCaAV0AL5S1d3AQmB4ufcdDUxT1WIf4zC1mCUFE6heVtU9qroL+A74SVVXqmoh8DHQ07vfdcBnqvql96L2f0ADnItufyAUeEFVi1X1Q2Bpuc+4BXhdVX9S1VJVnQQUeo87LlVdqKo/q6pHVdfgJKZzvZuvBxao6lTv52ao6ioRCQJuBu5V1V3ez1zs/Z188YOqzvJ+5kFVXa6qP6pqiapux0lqh2L4HbBbVf+lqgWqmqOqP3m3TcJJBIhIMDASJ3EaY0nBBKw95R4frOR5pPdxK2DHoQ2q6gFSgDjvtl165KyPO8o9bgs84K1+yRSRTCDee9xxiUg/EfnGW+2SBdyGc8eO9z2SKzksBqf6qrJtvkipEEMnEflURHZ7q5T+7kMMALOBLiLSHqc0lqWqS04xJlPLWFIwNV0azsUdABERnAviLuBXIM772iFtyj1OAf6fqjYu9xOuqlN9+NwpwBwgXlWjgNeAQ5+TAiRWcsw+oOAY2/KA8HK/RzBO1VN5Fac0ngBsADqqaiOc6rUTxYCqFgAzcEo0Y7BSginHkoKp6WYAQ0XkfG9D6QM4VUCLgR+AEuAeEQkRkauBvuWOfRO4zXvXLyIS4W1AbujD5zYE9qtqgYj0BUaV2zYZuEBEhns/N1pEenhLMW8Dz4lIKxEJFpEB3jaMTUCY9/NDgUeBE7VtNASygVwROR24vdy2T4EWInKfiNQXkYYi0q/c9neBm4BhwPs+/L6mjrCkYGo0Vd2IUz/+Ms6d+OXA5apapKpFwNU4F78DOO0PH5U7dhlOu8Ir3u1bvPv64g7gKRHJAR7DSU6H3ncncBlOgtqP08h8pnfzeOBnnLaN/cA/gSBVzfK+50ScUk4ecERvpEqMx0lGOTgJbnq5GHJwqoYuB3YDm4Eh5bb/F6eBe4W3PcIYAMQW2TGmbhKRr4EpqjrR7VhM4LCkYEwdJCJ9gC9x2kRy3I7HBA6rPjKmjhGRSThjGO6zhGAqspKCMcaYMlZSMMYYU6bGTaoVExOjCQkJbodhjDE1yvLly/epasWxL0epcUkhISGBZcuWuR2GMcbUKCKy48R7WfWRMcaYciwpGGOMKWNJwRhjTJka16ZQmeLiYlJTUykoKHA7FL8KCwujdevWhIbaWijGGP+oFUkhNTWVhg0bkpCQwJETYtYeqkpGRgapqam0a9fO7XCMMbVUrag+KigoIDo6utYmBAARITo6utaXhowx7qoVSQGo1QnhkLrwOxpj3FUrqo+MMabWKT4IWbsgOxWyUp3HHS+EuF5+/VhLClUgMzOTKVOmcMcdd5zUcZdddhlTpkyhcePGforMGBOQPKWQu8d7sff+ZO/yPk5xEkD+vqOPi4i2pFATZGZm8u9///uopFBaWkpwcPAxj5s7d66/QzPGVDdVKMgqd6FPOXynfygB5KSBp+TI4+o1hMbx0CgOWvWCqNaHfxrFQaNWEHKixfh+O0sKVeChhx4iOTmZHj16EBoaSmRkJC1btmTVqlWsX7+eK6+8kpSUFAoKCrj33nsZN24ccHjKjtzcXC699FLOOussFi9eTFxcHLNnz6ZBgwYu/2bGmKOUFJa7qy93d59d7qJflHvkMUGhzkU9Kh7aDjh8oY+K91744yAsqmx3VWV3dgHJe/NITstl6+pcktNX8cez2jHk9GZ+/fVqXVJ48pN1rE/LrtL37NKqEY9ffsYxt//jH/9g7dq1rFq1ioULFzJ06FDWrl1b1nX07bffpmnTphw8eJA+ffpwzTXXEB0dfcR7bN68malTp/Lmm28yfPhwZs6cyejRo6v09zDGnIDHA3nplVzoUw4ngLy9Rx8XEetc3KM7QPshhy/0hy76Ec0g6Oh+PQXFpezIyCd5cy7Je/eSnJ5LcnoeW9NzySsqLduvYf0Q2jeLpKjU48/fHqiFSSEQ9O3b94ixBC+99BIff/wxACkpKWzevPmopNCuXTt69OgBQO/evdm+fXu1xWtMnVGQXfmFPivV26C7CzzFRx4TGnG4GqdF18MX+kZxh/8NDTvmR6oq+/OKSE7Pcy76e3PLLv4pB/Ipv6RNXOMGJDaLZHhCPImxkbSPjaBDbCSxDetXW+/DWpcUjndHX10iIiLKHi9cuJAFCxbwww8/EB4ezuDBgysda1C//uG6wuDgYA4ePFgtsRpTa5QUOXX1lVbpeP8tzDryGAn2Vuu0htZ9oMuVR9blR7WGsMbgwwW5pNTDzv35R138t+7LIzP/cKKpHxJE+9hIureO4qqecSQ2iyQxNoJ2MRGE13P/kux+BLVAw4YNycmpfFXDrKwsmjRpQnh4OBs2bODHH3+s5uiMqQVUIW/f0XX35Xvu5OwGKqwk2aCpc2FvkgAJg8o13Hr/bdgCgo7dGaQy2QXF3gu+U81z6K5/R0YexaWHPz+2YX0SYyMY2q0libGRJDaLpH1MBHGNGxAUFLhjjiwpVIHo6GgGDRpE165dadCgAc2bNy/bdskll/Daa6/RvXt3TjvtNPr37+9ipMYEqMLcoy/2FXvulBYeeUxIA2+9fWvocP7hC335Hjv1wk8pHI9HScs66Nz1l1X3OBf/9JzDcYQECW2jw0mMjeTCLs2di39sBO1jI4lqUDPnKKtxazQnJSVpxUV2fvnlFzp37uxSRNWrLv2uphYqKYJfV8OO/0LqUjiww7nwF2QeuZ8EQcOWh+vtyzfaHuq1E97Up2qd4zlYVMrWfbkVLv55bNuXS0Hx4UbdRmEhdGgWWXbHf+jiH980nNDgmjExhIgsV9WkE+1nJQVjjP8UH4TUZbBj8eFEUJzvbIvu4Py06Xd098yGLSG4au60VZX0nEK2pOey9VB9vzcJ7Mo83HYnAvFNwkmMjWBgYnTZhT+xWSTREfXqzDQzlhSMMVWnIBtSljgJYMdi2LXc25tHnJ47vW6AtgOhzUCIPOFywSelqMTDzv15bNmbd0R1z9a9ueQUHh4o1iA0mMRmESQlNOG62Hjv3X8ECdERhIWeXPtCbWRJwRhz6vIyYOcPh0sCu9eAeiAoBFr1hAF3QNtBEN8PGlTNdC6Z+UXe3j1HXvx37s+n1HO4OrxFozASm0VwVa84712/08WzRaOwgG7odZslBWOM77LTvAnA+5P+i/N6SJjTpfOcB52SQOs+UC/i+O91HKUeJfVAftnFf+u+w0kgI6+obL96wUEkxIRzeouGTi+fZhHei38kkfXt8nYq7KwZYyqnCge2l0sC/4UD25xt9Ro6bQHdf++UBFr1PKV5eXILS9h6RF2/c/HflpFHUcnhht6mEfVIjI043MPHe/Fv3SScYLvrr1KWFIwxDlVI33i4PWDHYmcwGDj9/dsOhL63OP827wbBp3b5yC4o5v0fdzDlp52kHjjc0Bsk0DY6gvYxEZx7WqzTyOu9628aUa8qfkPjA0sKVeBUp84GeOGFFxg3bhzh4afWn9qYU+Yphd0/Hy4F7PwB8jOcbZEtnMFebQc6JYGY0yqdu+dk7M0p4D//3c77P+wgp7CEQR2iGdm3TdnFv010OPVDrKHXbZYUqsCxps72xQsvvMDo0aMtKRj/KymCtJWHSwI7f4Qi70j8JgnQ6RJvEhgITdr95jEAh+zIyOONb7fywfJUiks9XNa1JbcPTqRrXNSJDzbVzpJCFSg/dfaFF15Is2bNmDFjBoWFhVx11VU8+eST5OXlMXz4cFJTUyktLeVvf/sbe/bsIS0tjSFDhhATE8M333zj9q9iapOifGdcQPkxAiXeebdiTz/cHtBmgDM2oIqtT8vmtUXJfLomjZCgIK7pHce4cxJpF3PqDdDG/2pfUvj8IadIXJVadINL/3HMzeWnzp4/fz4ffvghS5YsQVUZNmwY3377Lenp6bRq1YrPPvsMcOZEioqK4rnnnuObb74hJiamamM2dU9BFuz86XBJIG2Fs5CLBDl/w0k3e8cIDIAI//y9qSpLtx9gwsItfLMxnYh6wYw9uz1/PKsdzRsdeyZREzhqX1Jw2fz585k/fz49e/YEIDc3l82bN3P22Wczfvx4/vKXv/C73/2Os88+2+VITY2Xt+/InkG7fwbUWdAlrhcMvNs7RqDvEQu4+IPHo3y9YS8TFiWzfMcBmkbU44ELO3HDgASiwmvmHEB1lV+TgohcArwIBAMTVfUfFba3Bd4GYoH9wGhVTf1NH3qcO/rqoKo8/PDD3HrrrUdtW758OXPnzuXhhx/moosu4rHHHnMhQlNjZaXCjh8OlwT2bXReD2kA8X1g8ENOSSAu6ZQngjtZxaUePl2TxmsLt7JxTw5xjRvw5LAzGJ4UT4N61mhcE/ktKYhIMPAqcCGQCiwVkTmqur7cbv8HvKuqk0TkPOAZYIy/YvKX8lNnX3zxxfztb3/j+uuvJzIykl27dhEaGkpJSQlNmzZl9OjRREZG8s477xxxrFUfmSOowv6tR5YEMnc42+o3gjb9ocdIpyTQsgeEVG+XzYNFpcxYlsIb325lV+ZBOjWP5LnhZ3L5ma1qzARxpnL+LCn0Bbao6lYAEZkGXAGUTwpdgPu9j78BZvkxHr8pP3X2pZdeyqhRoxgwYAAAkZGRvP/++2zZsoUHH3yQoKAgQkNDmTBhAgDjxo3j0ksvpWXLltbQXJd5PJC+4cgxArm7nW3h0U4JoP/t3jECXU96DYCqkpVfzHs/buc//91ORl4Rvdo05slhZ3De6c1s6ohawm9TZ4vItcAlqjrW+3wM0E9V7yq3zxTgJ1V9UUSuBmYCMaqacaz3tamz687vWquVljjzBB1KADsXw8EDzraGrSqMEehUZd1DT9We7ALe/n4bk3/aSW5hCYNPi+X2cxPp265pnZk9tKYLhKmzK/tLqZiBxgOviMhNwLfALqCk4kEiMg4YB9CmTZuqjdKY6lBSCLtWHC4JpPwERbnOtqbt4fShTgJoOxAat3U9CRyybV8eb3ybzMzluyjxeBjavRW3ndueM1rZGIPayp9JIRWIL/e8NZBWfgdVTQOuBhCRSOAaVa2wiCqo6hvAG+CUFPwVsDFVpijPO4W0tySQuvTwymHNusCZIw5PId2opbuxVmLtriwmLErm859/JSQ4iN8ntWbcOe1pG21jDGo7fyaFpUBHEWmHUwIYAYwqv4OIxAD7VdUDPIzTE+mUqGqtL8bWtFXy6hSPB7YsgO3fOUng11WHxwi0PPPwnEFtBjgrhgUgVeXHrfuZsCiZbzelE1k/hHHnJHLzoASa2RiDOsNvSUFVS0TkLmAeTpfUt1V1nYg8BSxT1TnAYOAZEVGc6qM7T+WzwsLCyMjIIDo6utYmBlUlIyODsDD7cgacjGSYfaczd1BwPYjrDYPudZJAfD+o39DtCI/L41G+/GUPExYmsyolk5jIejx48WmM7t+2xq4zbE5drVijubi4mNTUVAoKClyKqnqEhYXRunVrQkPtixoQPB5Y8joseNLpEnrx36HrNRDawO3IfFJc6mH2qjReW5TMlr25xDdtwLhzEvl979a2AlktFAgNzdUmNDSUdu3auR2GqUvKlw46XgyXvxiQbQOVyS8qYfrSFN78ditpWQWc3qIhL47owdBuLQmxMQZ1Xq1ICsZUm/Klg+B6cOUEOHNkwPQWOp7M/CImLd7BO4u3cSC/mD4JTfh/V3Vj8Gmxtbba1Zw8SwrG+Gr/Vph9l9OttONF3tJBK7ejOqFfsw7y1nfbmLJkJ/lFpZx/ejNuG5xIn4TAbPA27rKkYMyJeDyw9E1Y8IQz2dwV/4YeowK+dJCcnsvri5L5eOUuPAqXd2/JbYMTOb1FI7dDMwHMkoIxx7N/m7d08D10uNApHfhh7YGqtCY1kwkLk/li3W7qBQcxok8bxp3TnvimtpCTOTFLCsZUxuOBpRNhweMQFALDXoGeowO2dKCqLE7OYMLCZL7fso+GYSHcMTiRmwa2I7ZhfbfDMzWIJQVjKtq/Debc7QxESzwfhr0EUa3djqpSHo8yf/1uJixMZnVqFrEN6/PQpadzfb82NAyzrsvm5FlSMOYQjweWvQVfPu7MQjrsZeg5JiBLB0UlHmat3MVr3yazNT2PttHh/P2qblzdK87GGJjfxJKCMQAHtjttB9u/g8TznIQQgKWDvMISpi7ZycTvtrE7u4AuLRvx8sieXNatJcE2dbWpApYUTN3m8cDyt2H+Y848RZe/BL1uCLjSwf68IiYt3s6kH7aTmV9Mv3ZN+ee13TmnY4yNMTBVypKCqbsO7IA5d8G2b6H9EKd00Dj+xMdVo12ZB5n43VamLUnhYHEpF3Zpzm3nJtK7bRO3QzO1lCUFU/eowrK34cvHAHG6mfa6MaBKB1v25vDaoq3MWrkLgGE9WnH7uYl0bB7Yk+uZms+SgqlbMnc6bQfbFkH7wU5X0wAqHaxKyeTf32xh/vo9hIUGMbp/W8ae3Y7WTWyMgakelhRM3aAKy/8D8//mPP/dC9D7poAoHagq323ex4SFyfywNYNGYSHcc14HbhyYQHSkjTEw1cuSgqn9Mnc64w62LoR258IVr0Bj95d1LfUoX6zdzYRFW1i7K5vmjerzyGWdGdmvDZH17atp3GF/eab2UoXl73hLBwpDn4Okm10vHRSWlPLRil28viiZ7Rn5tIuJ4J/XdOPKnnHUD7ExBsZdlhRM7ZSZ4i0dfAPtznHaDpq0dTWk3MISpvy0g4nfbWNvTiHd4qL49/W9uPiMFjbGwAQMSwqmdlGFFZNg3qOgHhj6L+h9MwS5t3hMRm4h7yzezqTF28kuKGFgYjTPDe/BoA61d/lYU3NZUjC1R1YqzLkHkr+ChLOdtoMmCa6Fk3ognze/3cr0ZSkUlni4uEsLbhucSI/4xq7FZMyJWFIwNZ8qrHwP5j0CnlK47P8g6Y+ulQ427s7h9UXJzF6dhgBX9Yzj1nPb06GZjTEwgc+SgqnZsnbBJ/fAlgVO6WDYy9DUnfW6l+84wISFW1jwy14ahAZz08AE/nhWO1o1buBKPMacCksKpmZShZXvw7y/gqcELn0W+ox1pXSQX1TC05+uZ+qSFBqHh3LfBR25cUACTSLqVXssxvxWlhRMzZO1Cz65F7Z8CW0HOW0HTdu7Esr6tGzunrqCrfvyuPXc9tx7fkfC69nXytRc9tdrag5VWDUZvvgreIrh0v+FPre4UjpQVSYt3s7f524gKjyU927ux1kdY6o9DmOqmiUFUzNkpzmlg83zoc1AuPJV10oHGbmF/PnDNXy1YS/nnd6MZ6/tbtNRmFrDkoIJbKqwagp88TCUFsEl/4S+41zrWfTfLfu4f/oqMvOLefzyLtw0MMHGGphaxZKCCVzZafDJfbB5HrQZAFe8CtGJroRSXOrhuS838dqiZNrHRPCfP/ThjFZRrsRijD9ZUjCBRxVWT4UvHoKSIrjkH9D3VtdKBzsz8rl72kpWp2Qyok88j13exRqTTa1lf9kmsGT/Cp/eB5u+cL10ADB71S4e+XgtIvDqqF4M7d7StViMqQ6WFExgUIU10+HzPzulg4ufgX63QpA7s4bmFpbw+Ox1zFyRSu+2TXhxRA9b6MbUCZYUjPtydjttB5s+h/j+cOW/XS0d/Jyaxd1TV7Bzfz73nN+Re87rQEiwexPqGVOdLCkY96jCmhne0kEBXPT/oP/trpUOPB7lre+38b/zNhATWZ8pt/Snf/toV2Ixxi2WFIw7cvY4bQcb50Lrvk7pIKaja+HszSnggRmr+W7zPi4+ozn/vKY7jcNtmgpT91hSMNVLFX7+AOY+6C0d/A/0v8O10gHAwo17Gf/BanIKSvifK7tyfb82NvbA1FmWFEz1ydkDn/0JNnwKrfvAlRNcLR0UlXh4dt4G3vxuG6c1b8jksf05rYVNb23qNksKxv9U4ecP4fMHoSgfLnwaBtzpaulg27487pm6kp93ZTGmf1seGdqZsFBbH9kYSwrGv3L3wqf3O6WDuCSndBDbybVwVJWZK3bx2Oy11AsJ4vUxvbn4jBauxWNMoPFrUhCRS4AXgWBgoqr+o8L2NsAkoLF3n4dUda4/YzLVRBXWzoS5453SwQVPwsC7XS0d5BQU8+istcxelUa/dk15YUQPWkbZAjjGlOe3pCAiwcCrwIVAKrBUROao6vpyuz0KzFDVCSLSBZgLJPgrJlNNcvc6bQe/fAJxvb2lg9NcDWnlzgPcM20laZkFPHBhJ+4Y0oHgIGtMNqYif5YU+gJbVHUrgIhMA64AyicFBRp5H0cBaX6Mx/ibKqz7CD4bD0W5cMETMOBuCHavltLjUV77Npnn5m+ieaMwZtzan95tm7oWjzGBzp/f1jggpdzzVKBfhX2eAOaLyN1ABHBBZW8kIuOAcQBt2rSp8kBNFchN95YO5kCrXk7poNnproa0J7uA+6evYnFyBkO7t+TvV3UjqkGoqzEZE+j8mRQqK5trhecjgXdU9V8iMgB4T0S6qqrniINU3wDeAEhKSqr4HsZtaz9y2g4Kc+D8x2HgPa6WDgC++mUP4z9YTUGxh/+9pju/T2ptYw+M8YE/v7mpQHy55605unroj8AlAKr6g4iEATHAXj/GZapK3j6ndLB+NrTq6S0ddHY1pILiUv7x+QbeWbydLi0b8dLInnRoFulqTMbUJP5MCkuBjiLSDtgFjABGVdhnJ3A+8I6IdAbCgHQ/xmSqyrqP4bMHvKWDx2Dgva6XDrbszeGuKSvZsDuHmwe14y+Xnkb9EBt7YMzJ8Nu3WFVLROQuYB5Od9O3VXWdiDwFLFPVOcADwJsicj9O1dJNqmrVQ4Esb59TVbTuY2jZwykdNO/iakiqyrSlKTz5yTrC64Xw9k1JnHd6c1djMqam8uutnXfMwdwKrz1W7vF6YJA/YzBVaP1s+PRPUJAF5/0NBt3neukgK7+Yhz9ew9yfd3NWhxieG34mzRqFuRqTMTWZjWg2J5aX4S0dfOSUDm78xPXSAcCy7fu5d9oq9mQX8NClpzPu7PYE2dgDY34TSwrm+MqXDoY8CmfdB8Huduss9SivfL2FF7/aRHzTcD68fSA94hu7GpMxtYUlBVO5g5lOz6K1M6FFd7hhNrTo6nZUpGUe5L7pq1iybT9X9YzjqSvOoGGYjT0wpqpYUjBHy0iGKdfBgW0w5BE4637XSwcAX6zdzV9mrqGk1MNzw8/k6l6t3Q7JmFrHp6QgIjOBt4HPKw4sM7XMtm9h+hiQIKftoO1AtyOioLiUpz9dz+SfdtItLoqXRvakXUyE22EZUyv5WlKYAPwBeElEPsAZhbzBf2EZVyx/xxl7EN0BRk6Dpu3cjoiNu3O4e+oKNu3J5dZz2vPARadRLyTI7bCMqbV8SgqqugBYICJROFNTfCkiKcCbwPuqWuzHGI2/eUph/qPw47+hwwVw7dsQFuVqSKrK+z/u4H8++4WGYaG8e3NfzukU62pMxtQFPrcpiEg0MBoYA6wEJgNnATcCg/0RnKkGBdkw84+weT70u91ZM9nlsQcH8or488w1fLl+D+d2iuVfw88kJrK+qzEZU1f42qbwEXA68B5wuar+6t00XUSW+Ss442cHtsOUEbBvE/zueUi62e2I+CE5g/unryIjr5BHh3bm5kHtbOyBMdXI11vCV1T168o2qGpSFcZjqsuOH2D69eApgTEfQfvBroZTUurhxa8288o3W2gXHcHEGwfRNc7dKixj6iJfk0JnEVmhqpkAItIEGKmq//ZfaMZvVk2FT+6BqHgYNQNiOrgaTsr+fO6dtpIVOzP5fe/WPDHsDCLqW29pY9zg6zfvFlV99dATVT0gIrcAlhRqEo8Hvn4Kvn8e2p0Dv58E4e6uQvbJ6jT++vHPoPDSyJ4MO7OVq/EYU9f5mhSCREQOzWDqXX+5nv/CMlWuMBc+vhU2fAq9/wCXPevqgLT8ohKemLOOGctS6dmmMS+N6El803DX4jHGOHxNCvOAGSLyGs4U17cBX/gtKlO1slJh6gjYsw4u+Sf0uxVcXIVs7a4s7pm2km378rhrSAfuvaAjocE29sCYQOBrUvgLcCtwO84ym/OBif4KylSh1OUwbSQU5TvtBx0vdC0UVeXt/27nn59voElEKJPH9mNgYoxr8Rhjjubr4DUPzqjmCf4Nx1Spnz+E2XdCZHNnQjsXl8rcl1vIgx+s5puN6VzQuTn/e213mkZYDaQxgcbXcQodgWeALjhLZgKgqu39FJf5LVRh4TOw6J/QZgBc9z5EuHdH/t3mdP40YzVZB4t56oozGNO/LeJi9ZUx5th8rT76D/A48DwwBGceJPtWB6LigzDrdme5zB7XO4PSQtwZDVwISxVJAAAYf0lEQVRU4uFfX27k9UVb6dgskndv7kvnlo1cicUY4xtfk0IDVf3K2wNpB/CEiHyHkyhMoMjZDVNHQtpKuPApGHiPaw3KOzLyuGfqSlanZjGqXxv+NrQLDeoFuxKLMcZ3viaFAhEJAjaLyF3ALqCZ/8IyJ+3X1c6UFQVZMGIynD7UtVBmrdzFIx//THCQ8NroXlzStaVrsRhjTo6vSeE+IBy4B3gapwrpRn8FZU7S+jnOGIQGTeGP86BFN1fCyC0s4bFZa/lo5S76JjTl+RE9iGvcwJVYjDGn5oRJwTtQbbiqPgjk4rQnmECgCt8/B189BXFJMGIKNGzuSihrUjO5Z+pKdu7P574LOnLXkA6E2NgDY2qcEyYFVS0Vkd7lRzSbAFBSCHPugTXToOu1cMWrEBp24uOqmMejvPndVp6dt5FmDesz/dYB9Elwd+oMY8yp87X6aCUw27vqWt6hF1X1I79EZY4vN92Z4TTlJxjyKJwz3pUG5b05BTwwYzXfbd7HpV1b8I+ruxMV7v5azsaYU+drUmgKZADnlXtNAUsK1W3POqdBOS8dfv8OnHGVK2F8s3Ev42esJq+ohGeu7saIPvE29sCYWsDXEc3WjhAINn7hrJJWvyH8YS7E9ar2EApLSvnfLzby1vfbOL1FQ6aN7E/H5g2rPQ5jjH/4OqL5PzglgyOoqvtLddUFqvDDq846yi27w8hp0Kj6p5hOTs/lnqkrWZeWzU0DE3jo0tMJC7WxB8bUJr5WH31a7nEYcBWQVvXhmKOUFMHcB2DFu9B5GFz1GtSLqNYQVJUPlqfy+Ox1hIUGMfGGJC7o4k4vJ2OMf/lafTSz/HMRmQos8EtE5rD8/TB9DOz4Hs4eD0MegaDq7eaZXVDMIx+v5ZPVaQxoH83z1/WgRVT193IyxlSPU13zsCPQpioDMRWkb4IpwyE7Da5+E7oPr/YQlu84wL3TVvJrVgEPXnwat52bSHCQNSYbU5v52qaQw5FtCrtx1lgw/pD8Ncy4CULqwU2fQnzfag/h0zVp3DttFa0ah/HBbQPo1aZJtcdgjKl+vlYfWfeS6rLkTfj8LxB7OoyaBo2rv0C2JjWTB2aspmd8Y97+Qx8ahdnYA2PqCp8qqEXkKhGJKve8sYhc6b+w6qDSEvhsPMwd76yO9sd5riSEPdkF3PLuMmIi6/PamN6WEIypY3xttXxcVbMOPVHVTGza7KpzMBMmXwtL33Smux4xxRmLUM0KiksZ995ycgpKePOGJGIi3VmHwRjjHl8bmitLHqfaSG3Ky0iGqSNg/zYY9gr0GuNKGKrKQzPXsDolk9dG96ZLK1sMx5i6yNcL+zIReQ54FafB+W5gud+iqiu2fQczxgACN8yChLNcC2XComRmrUpj/EWduKRrC9fiMMa4y9fqo7uBImA6MAM4CNzpr6DqhBXvwntXQkQzuOUrVxPCl+v38Oy8jVx+ZivuHNLBtTiMMe7ztfdRHvDQyb65iFwCvAgEAxNV9R8Vth9a8xmcRXyaqWrjk/2cGsVTCl8+Bj+8Aonnw+//A2FRJz7OTzbszua+aSvpFhfFs9d2t0ntjKnjfO199KWINC73vImIzDvBMcE41U2XAl2AkSLSpfw+qnq/qvZQ1R7Ay9T2WVcLsp01lH94BfreCqNmuJoQMnILGTtpGRH1Q3hjTJLNY2SM8blNIcbb4wgAVT0gIidao7kvsEVVtwKIyDTgCmD9MfYfSW3u0XRgh9OgnL4Rhv4L+ox1NZyiEg+3T17B3pxCZtw6wKauMMYAvrcpeESkrNO8iCRQyaypFcQBKeWep3pfO4qItAXaAV8fY/s4EVkmIsvS09N9DDmA7PwR3jwPsnfB6JmuJwRV5fE5a1mybT/PXtudHvG1u8bOGOM7X0sKjwDfi8gi7/NzgHEnOKayyuljJZIRwIeqWlrZRlV9A3gDICkpqWYtCbp6Gsy5G6LiYdR0iOnodkRMWrydqUtSuGNwIlf0qDRPG2PqKF8bmr8QkSScRLAKmI3TA+l4UoH4cs9bc+zptkdQ23ozeTzw9dPw/XOQcDYMfxfC3V+7+LvN6Tz92S9c0Lk54y86ze1wjDEBxtcJ8cYC9+Jc2FcB/YEfOHJ5zoqWAh1FpB2wC+fCP6qS9z4NaOJ9v9qhKA8+GgcbPoVeNzptCMHuTxexNT2XOyevoENsJC+M6EGQzXhqjKnA1zaFe4E+wA5VHQL0BI5bua+qJcBdwDzgF2CGqq4TkadEZFi5XUcC01S1ZlULHUvWLnj7Etg4Fy5+Bi5/MSASQtbBYsa+u4yQ4CAm3phEZH0bkG6MOZqvV4YCVS0QEUSkvqpu8N7hH5eqzgXmVnjtsQrPn/A52kC3a7nT5bQoH0ZOh04XuR0RACWlHu6eupKdGflMHtuP+KbhbodkjAlQviaFVO84hVnAlyJyAFuO80hrP4JZt0NkM7hhNjTr7HZEZZ75fAPfbkrnmau70a99tNvhGGMCmK8NzVd5Hz4hIt8AUcAXfouqJlGFRf+Ehc9AmwFw3fsQEeN2VGVmLE3hre+3cdPABEb2tcXyjDHHd9IVy6q66MR71RHFB2H2nbB2Jpw5Ci5/AUICZ7rppdv388isnzm7YwyPDg2ckosxJnBZa+OpytkN00bBrhVwwZMw6F4IoHmDUg/kc9t7y2ndJJxXRvYiJNjXPgXGmLrMksKp+HW106B88IBTXdT5d25HdIS8whLGTlpGUamHiTcmERXufu8nY0zNYEnhZP3yKXx0CzRoAjfPg5bd3Y7oCB6P8qcZq9i0J4f//KEvibGRbodkjKlBrE7BV6rw/fMwfbTTs+iWbwIuIQA8v2AT89bt4ZGhXTi3U6zb4RhjahgrKfiipBA+uRdWT4Wu18AVr0JoA7ejOsonq9N4+estXJcUz82DEtwOxxhTA1lSOJG8fTDtekj5EQb/Fc79c0A1KB+yJjWT8R+spk9CE56+sqstlmOMOSWWFI5nz3qYeh3k7oVr/wNdr3Y7okrtyS7glneXERNZnwmje1MvxGoFjTGnxpLCsWyaDx/eDPUi4A9zIa632xFVqqC4lHHvLSenoISZtw8kJjJwxkkYY2oeu6WsSBV++LdTQmjaDm75OmATgqry0Mw1rE7J5PnretC5ZSO3QzLG1HBWUiivtBg+ewBWTILOl8NVrzslhQA1YVEys1alMf6iTlx8Rgu3wzHG1AKWFA7J3w8zboDt38HZD8CQRyEocAtSX67fw7PzNnL5ma24c0gHt8MxxtQSlhQA9m2GKcMhKxWuegPOvM7tiI5rw+5s7pu2km5xUTx7bXfraWSMqTKWFJK/gQ9uhKBQuPFTaNPP7YiOKyO3kLGTlhFRP4Q3xiQRFhrsdkjGmFokcOtHqsPSifD+NdAoDsZ9E/AJoajEw+2TV7A3p5A3bkiiRVSY2yEZY2qZullSKC2BeX+FJa9Dp0vgmolQv6HbUR2XqvL4nLUs2bafF0f0oEd8Y7dDMsbUQnUvKRRkwQd/gOSvYMBdcOFTEBT4VTCTFm9n6pIU7hicyBU94twOxxhTS9WtpLB/K0wZAfuTYdjL0OsGtyPyyXeb03n6s1+4oHNzxl90wqWxjTHmlNWdpLD9v84Mp6izhnLCWW5H5JOt6bncOXkFHWIjeWFED4KCrKeRMcZ/6k5SyE6DiFgYORWiE92OxidZB4sZ++4yQoKDmHhjEpH1685/lzHGHXXnKtP999BlWECtoXw8JaUe7p66kp0Z+Uwe24/4puFuh2SMqQPqTlKAGpMQAJ75fAPfbkrnmau70a99tNvhGGPqiLo9TiFAzViawlvfb+OmgQmM7NvG7XCMMXWIJYUAs3T7fh6Z9TNnd4zh0aGd3Q7HGFPHWFIIIKkH8rntveW0bhLOKyN7ERJs/z3GmOplV50AkVdYwthJyygq9TDxxiSiwkPdDskYUwdZUggAHo9y//RVbNqTwyujepEYG+l2SMaYOsqSQgB4fsEm5q/fwyNDu3Bup1i3wzHG1GGWFFz2yeo0Xv56C9clxXPzoAS3wzHG1HGWFFy0JjWT8R+spk9CE56+sqstlmOMcZ0lBZfsyS7glneXERNZnwmje1MvxP4rjDHuq1sjmgNEQXEp495bTk5BCTNvH0hMZM0ZaW2Mqd0sKVQzVeWhmWtYnZLJ62N607llI7dDMsaYMlZnUc0mLEpm1qo0xl/UiYvPaOF2OMYYcwRLCtXoy/V7eHbeRi4/sxV3DungdjjGGHMUvyYFEblERDaKyBYReegY+wwXkfUisk5EpvgzHjdt2J3NfdNW0i0uimev7W49jYwxAclvbQoiEgy8ClwIpAJLRWSOqq4vt09H4GFgkKoeEJFm/orHTRm5hYydtIyI+iG8MSaJsNDAXxPaGFM3+bOk0BfYoqpbVbUImAZcUWGfW4BXVfUAgKru9WM8rigq8XD75BXszSnkjRuSaBEV5nZIxhhzTP5MCnFASrnnqd7XyusEdBKR/4rIjyJySWVvJCLjRGSZiCxLT0/3U7hVT1V5fM5almzbz7PXdqdHfGO3QzLGmOPyZ1KorNJcKzwPAToCg4GRwEQROerKqapvqGqSqibFxtacuYEmLd7O1CUp3DE4kSt6VMyHxhgTePyZFFKB+HLPWwNplewzW1WLVXUbsBEnSdR4321O5+nPfuGCzs0Zf9FpbodjjDE+8WdSWAp0FJF2IlIPGAHMqbDPLGAIgIjE4FQnbfVjTNVia3oud05eQYfYSF4Y0YOgIOtpZIypGfyWFFS1BLgLmAf8AsxQ1XUi8pSIDPPuNg/IEJH1wDfAg6qa4a+YqkPWwWLGvruMkOAgJt6YRGR9GzRujKk5/HrFUtW5wNwKrz1W7rECf/L+1HglpR7unrqSnRn5TB7bj/im4W6HZIwxJ8VuY6vQM59v4NtN6TxzdTf6tY92OxxjjDlpNs1FFZmxNIW3vt/GTQMTGNm3jdvhGGPMKbGkUAWWbt/PI7N+5uyOMTw6tLPb4RhjzCmzpPAbpR7I57b3ltO6STivjOxFSLCdUmNMzWVXsN8gr7CEsZOWUVTqYeKNSUSFh7odkjHG/CaWFE6Rx6PcP30Vm/bk8MqoXiTGRrodkjHG/GaWFE7R8ws2MX/9Hh4Z2oVzO9WcqTeMMeZ4LCmcgjmr03j56y1clxTPzYMS3A7HGGOqjCWFk7QmNZMHP1hNn4QmPH1lV1ssxxhTq1hSOAl7sgu45d1lxETWZ8Lo3tQLsdNnjKldbESzjwqKSxn37jJyCkqYeftAYiLrux2SMcZUOUsKPlBV/jJzDatTs3h9TG86t2zkdkjGGOMXVv/hgwmLkpm9Ko3xF3Xi4jNauB2OMcb4jSWFE/hy/R6enbeRy89sxZ1DOrgdjjHG+JUlhePYsDub+6atpFtcFM9e2916Ghljaj1LCseQkVvI2EnLiKgfwhtjkggLDXY7JGOM8TtraK5EUYmH2yevYG9OITNuHUCLqDC3QzLGmGphJYUKVJXH56xlybb9PHttd3rEN3Y7JGOMqTaWFCqYtHg7U5ekcMfgRK7oEed2OMYYU60sKZTz3eZ0nv7sFy7s0pzxF53mdjjGGFPtLCl4bU3P5c7JK+gQG8nz1/UgKMh6Ghlj6h5LCkDWwWLGvruMkOAgJt6YRGR9a383xtRNdT4plJR6uHvqSnZm5DPh+l7ENw13OyRjjHFNnb8lfubzDXy7KZ1nru5Gv/bRbodjjDGuqtMlhRlLU3jr+23cNDCBkX3buB2OMca4rs4mhaXb9/PIrJ85u2MMjw7t7HY4xhgTEOpkUkg9kM9t7y2ndZNwXhnZi5DgOnkajDHmKHXuaphXWMLYScsoKvUw8cYkosJD3Q7JGGMCRp1KCh6Pcv/0VWzak8Mro3qRGBvpdkjGGBNQ6lRSeH7BJuav38MjQ7twbqdYt8MxxpiAU2eSwpzVabz89RauS4rn5kEJbodjjDEBqc4khZjIelzYpTlPX9nVFssxxphjqDOD1wYmxjAwMcbtMIwxJqDVmZKCMcaYE7OkYIwxpowlBWOMMWX8mhRE5BIR2SgiW0TkoUq23yQi6SKyyvsz1p/xGGOMOT6/NTSLSDDwKnAhkAosFZE5qrq+wq7TVfUuf8VhjDHGd/4sKfQFtqjqVlUtAqYBV/jx84wxxvxG/kwKcUBKueep3tcqukZE1ojIhyISX9kbicg4EVkmIsvS09P9Easxxhj8mxQqGyGmFZ5/AiSoandgATCpsjdS1TdUNUlVk2JjbXoKY4zxF38OXksFyt/5twbSyu+gqhnlnr4J/PNEb7p8+fJ9IrLjFGOKAfad4rH+ZHGdHIvr5AVqbBbXyfktcbX1ZSd/JoWlQEcRaQfsAkYAo8rvICItVfVX79NhwC8nelNVPeWigogsU9WkUz3eXyyuk2NxnbxAjc3iOjnVEZffkoKqlojIXcA8IBh4W1XXichTwDJVnQPcIyLDgBJgP3CTv+IxxhhzYn6d+0hV5wJzK7z2WLnHDwMP+zMGY4wxvqtrI5rfcDuAY7C4To7FdfICNTaL6+T4PS5RrdghyBhjTF1V10oKxhhjjsOSgjHGmDK1Min4MBFffRGZ7t3+k4gkBEhcrkwQKCJvi8heEVl7jO0iIi95414jIr0CJK7BIpJV7nw9Vtl+VRxTvIh8IyK/iMg6Ebm3kn2q/Xz5GJcb5ytMRJaIyGpvXE9Wsk+1fx99jMu1CTtFJFhEVorIp5Vs8+/5UtVa9YPT/TUZaA/UA1YDXSrscwfwmvfxCJxJ+QIhrpuAV1w4Z+cAvYC1x9h+GfA5zij1/sBPARLXYODTaj5XLYFe3scNgU2V/D9W+/nyMS43zpcAkd7HocBPQP8K+7jxffQlLle+j97P/hMwpbL/L3+fr9pYUvBlIr4rODylxofA+eL/hZsDdoJAVf0WZ5zIsVwBvKuOH4HGItIyAOKqdqr6q6qu8D7OwRlwWXFOr2o/Xz7GVe285yDX+zTU+1Oxd0u1fx99jMsVItIaGApMPMYufj1ftTEp+DIRX9k+qloCZAHRARAX+DBBoAt8jd0NA7xVAJ+LyBnV+cHeYntPnLvM8lw9X8eJC1w4X96qkFXAXuBLVT3m+arG76MvcYE738cXgD8DnmNs9+v5qo1JwZeJ+HzZp6pV2QSBLnDjfPliBdBWVc8EXgZmVdcHi0gkMBO4T1WzK26u5JBqOV8niMuV86WqparaA2f+s74i0rXCLq6cLx/iqvbvo4j8DtirqsuPt1slr1XZ+aqNSeGEE/GV30dEQoAo/F9N4dMEgapa6H36JtDbzzH5ypdzWu1UNftQFYA6o+dDRSTG358rIqE4F97JqvpRJbu4cr5OFJdb56vc52cCC4FLKmxy4/t4wrhc+j4OAoaJyHacKubzROT9Cvv49XzVxqRQNhGfiNTDaYiZU2GfOcCN3sfXAl+rt9XGzbgq1Dv7NEFgNZkD3ODtVdMfyNLDExm6RkRaHKpLFZG+OH/PGcc/6jd/pgBvAb+o6nPH2K3az5cvcbl0vmJFpLH3cQPgAmBDhd2q/fvoS1xufB9V9WFVba2qCTjXiK9VdXSF3fx6vvw695Eb1LeJ+N4C3hORLTgZdkSAxOXKBIEiMhWnZ0qMiKQCj+M0vKGqr+HMX3UZsAXIB/4QIHFdC9wuIiXAQWBENST3QcAY4GdvfTTAX4E25eJy43z5Epcb56slMEmc5XmDgBmq+qnb30cf4wqYCTur83zZNBfGGGPK1MbqI2OMMafIkoIxxpgylhSMMcaUsaRgjDGmjCUFY4wxZSwpGFONxJmp9KiZL40JFJYUjDHGlLGkYEwlRGS0d779VSLyunfytFwR+ZeIrBCRr0Qk1rtvDxH50Ttx2sci0sT7egcRWeCdgG6FiCR63z7SO8HaBhGZXA0z9BrjM0sKxlQgIp2B64BB3gnTSoHrgQhghar2AhbhjLAGeBf4i3fitJ/LvT4ZeNU7Ad1A4NBUFz2B+4AuOOtrDPL7L2WMj2rdNBfGVIHzcSY/W+q9iW+AM72yB5ju3ed94CMRiQIaq+oi7+uTgA9EpCEQp6ofA6hqAYD3/Zaoaqr3+SogAfje/7+WMSdmScGYowkwSVUfPuJFkb9V2O94c8Qcr0qosNzjUux7aAKIVR8Zc7SvgGtFpBmAiDQVkbY435drvfuMAr5X1SzggIic7X19DLDIu5ZBqohc6X2P+iISXq2/hTGnwO5QjKlAVdeLyKPAfBEJAoqBO4E84AwRWY6z2tV13kNuBF7zXvS3cnhW1DHA694ZLouB31fjr2HMKbFZUo3xkYjkqmqk23EY409WfWSMMaaMlRSMMcaUsZKCMcaYMpYUjDHGlLGkYIwxpowlBWOMMWUsKRhjjCnz/wEOG+RMVoMg6wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accurancy:  0.9544264909901349\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'calm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-165-87726c845f4d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel_final\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[0mfinal_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfinal_predictions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreproc_english_sentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreproc_french_sentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menglish_tokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfrench_tokenizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-165-87726c845f4d>\u001b[0m in \u001b[0;36mfinal_predictions\u001b[1;34m(x, y, x_tk, y_tk)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'new jersey is sometimes calm during the fall and it is snowy in april'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx_tk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'post'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-165-87726c845f4d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'new jersey is sometimes calm during the fall and it is snowy in april'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx_tk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'post'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'calm'"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "def final_predictions(x, y, x_tk, y_tk):\n",
    "    \"\"\"\n",
    "    Gets predictions using the final model\n",
    "    :param x: Preprocessed English data\n",
    "    :param y: Preprocessed French data\n",
    "    :param x_tk: English tokenizer\n",
    "    :param y_tk: French tokenizer\n",
    "    \"\"\"\n",
    "    # TODO: Train neural network using model_final\n",
    "    # Save model, so that it can quickly load it in future (and perhaps resume training)\n",
    "    if os.path.exists(\"final_model.h5\") == False:\n",
    "        model_final.save(\"final_model.h5\")\n",
    "    \n",
    "    ## DON'T EDIT ANYTHING BELOW THIS LINE\n",
    "    score = model_final.evaluate(X_input, preproc_french_sentences, verbose=0)\n",
    "    print(\"Train accurancy: \", score[1])\n",
    "    y_id_to_word = {value: key for key, value in y_tk.word_index.items()}\n",
    "    y_id_to_word[0] = '<PAD>'\n",
    "\n",
    "    sentence = 'new jersey is sometimes calm during the fall and it is snowy in april'\n",
    "    sentence = [x_tk.word_index[word] for word in sentence.split()]\n",
    "    sentence = pad_sequences([sentence], maxlen=x.shape[-1], padding='post')\n",
    "    sentences = np.array([sentence[0], x[0]])\n",
    "    predictions = model_final.predict(sentences, len(sentences))\n",
    "\n",
    "    print('Sample 1:')\n",
    "    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[0]]))\n",
    "   # print('Il a vu un vieux camion jaune')\n",
    "    print('Sample 2:')\n",
    "    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[1]]))\n",
    "    print(' '.join([y_id_to_word[np.argmax(x)] for x in y[0]]))\n",
    "\n",
    "    return model_final\n",
    "\n",
    "final_model = final_predictions(preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
